{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s20033/MIW_projects/blob/main/Alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIrNRcd3HxJ5"
      },
      "source": [
        "<b><h1>Implementation of AlexNet</b></h1> <br>\n",
        "Using Keras Libraray implement AlexNet and train it to recognize fine grained classes from CIFAR100 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5OEMONxfQi4"
      },
      "source": [
        "#Library Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLer-foSHuA_"
      },
      "source": [
        "#import various libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "#from keras.optimizers import adam\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from keras.utils import np_utils # To transfor labels in categorical\n",
        "from keras.datasets import cifar100 # to load the dataset\n",
        "\n",
        "from keras import backend as K\n",
        "#K.set_image_dim_ordering('tf') # To tell Tensorflow the right order of dims\n",
        "# since I am using google colab and want to upload or download files\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3e6wMrSViNe"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import model_from_json\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uqnNzs0ca9x"
      },
      "source": [
        "<h2>Preparing the Data </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x532Vp82UU5j"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjcPug_wT90-"
      },
      "source": [
        "def process_images(image, label):\n",
        "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    # Resize images from 32x32 to 277x277\n",
        "    image = tf.image.resize(image, (227,227))\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvV_KfViUb4m"
      },
      "source": [
        "validation_images, validation_labels = train_images[:5000], train_labels[:5000]\n",
        "train_images, train_labels = train_images[5000:], train_labels[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDxGJloSXnyK",
        "outputId": "0a334bdb-3e04-488f-c043-155093fdcc93"
      },
      "source": [
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxupEjA3UnZU"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOtyszUHUyhg",
        "outputId": "8e983c15-763c-4ec8-e39b-adfc30d82c9f"
      },
      "source": [
        "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\n",
        "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
        "print(\"Training data size:\", train_ds_size)\n",
        "print(\"Test data size:\", test_ds_size)\n",
        "print(\"Validation data size:\", validation_ds_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size: 45000\n",
            "Test data size: 10000\n",
            "Validation data size: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo3EjynTfaf6"
      },
      "source": [
        "#Resize image size from (32, 32, 3) to (227, 227, 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppSnWCfiUzmw"
      },
      "source": [
        "train_ds = (train_ds\n",
        "                  .map(process_images)\n",
        "                  .shuffle(buffer_size=train_ds_size)\n",
        "                  .batch(batch_size=32, drop_remainder=True))\n",
        "test_ds = (test_ds\n",
        "                  .map(process_images)\n",
        "                  .shuffle(buffer_size=train_ds_size)\n",
        "                  .batch(batch_size=32, drop_remainder=True))\n",
        "validation_ds = (validation_ds\n",
        "                  .map(process_images)\n",
        "                  .shuffle(buffer_size=train_ds_size)\n",
        "                  .batch(batch_size=32, drop_remainder=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOb9huSpZc8p",
        "outputId": "211fd8c0-ca58-4469-e2c9-e1f8a6bbaf71"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((32, 227, 227, 3), (32, 1)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ5An4teI4W2"
      },
      "source": [
        "<h2> AlexNet Implementation. (Fused)</h2>\n",
        "This implementation is optimized for modern GPU Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZc2zGMLRui"
      },
      "source": [
        "Creating Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xylh3czmbp2p"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "                                 # Layer conv1\n",
        "    #keras.layers.ZeroPadding2D(padding=(0, 0), data_format=None),\n",
        "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
        "    # layer maxpool1\n",
        "    keras.layers.ZeroPadding2D(padding=(0, 0), data_format=None),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "    # layer conv2\n",
        "    keras.layers.ZeroPadding2D(padding=(2, 2), data_format=None),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu'),\n",
        "    # layer maxpool2\n",
        "    keras.layers.ZeroPadding2D(padding=(0, 0), data_format=None),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "    #layer conv3\n",
        "    keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None),\n",
        "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu'),\n",
        "   # layer conv4\n",
        "    keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None),\n",
        "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu'),\n",
        "    # layer conv5\n",
        "    keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu'),\n",
        "    #layer maxpool5\n",
        "    keras.layers.ZeroPadding2D(padding=(0, 0), data_format=None),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "    # layer flatten 6\n",
        "    keras.layers.Flatten(),\n",
        "    # layer flatten dense 7\n",
        "    keras.layers.Dense(4096, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(4096, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1000, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xik4m0kLRUWZ"
      },
      "source": [
        "<h2> Defining Model Structure </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8vjiRECbuVR",
        "outputId": "546d2731-8c9d-49c8-dd0d-14a50f17bf67"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 55, 55, 96)        34944     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPaddin (None, 55, 55, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 96)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_8 (ZeroPaddin (None, 31, 31, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 27, 27, 256)       614656    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_9 (ZeroPaddin (None, 27, 27, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_10 (ZeroPaddi (None, 15, 15, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 384)       885120    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPaddi (None, 15, 15, 384)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPaddi (None, 15, 15, 384)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 13, 13, 256)       884992    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_13 (ZeroPaddi (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4096)              37752832  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 62,378,344\n",
            "Trainable params: 62,378,344\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12OyAv_kb75K"
      },
      "source": [
        "history = model.fit(train_ds,\n",
        "          epochs=1,\n",
        "          validation_data=validation_ds,\n",
        "          validation_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq0p_5QzSw5A"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_layer_names=False, show_shapes=True, show_dtype=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1DWKFakLVZ4"
      },
      "source": [
        "Saving Model for later (In JSON)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itubobz3KqxO"
      },
      "source": [
        "# save model architecture in JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# save Model weight in Hdf5 \n",
        "model.save('model.h5'.format(1))\n",
        "model.save_weights(\"model.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C8_g9r9t_yc"
      },
      "source": [
        "# Tensor Board (optional)\n",
        "we’ll be utilizing TensorBoard for visualization and monitoring of our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPMsRDD0u1UB"
      },
      "source": [
        "root_logdir = os.path.join(os.curdir, \"logs\\\\fit\\\\\")\n",
        "def get_run_logdir():\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "run_logdir = get_run_logdir()\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJV9Bp6Yvmdv"
      },
      "source": [
        "In order to use tenor Board, open Terminal in directory level and run following comand:<br>\n",
        "[tensorboard --logdir logs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1RqSewTpRSn"
      },
      "source": [
        "# Loading Model and visualizing Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rnvScylpP9s"
      },
      "source": [
        "model = keras.models.load_model('model.h5'.format(1))\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "print('accuracy: {}'.format(acc*100))\n",
        "print('loss: {}'.format(loss))\n",
        "\n",
        "# result\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.title('model 1')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}